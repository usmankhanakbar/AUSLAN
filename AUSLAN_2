{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14447230,"sourceType":"datasetVersion","datasetId":9228384}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2026-01-13T19:02:38.411441Z","iopub.execute_input":"2026-01-13T19:02:38.411668Z","iopub.status.idle":"2026-01-13T19:02:59.385219Z","shell.execute_reply.started":"2026-01-13T19:02:38.411635Z","shell.execute_reply":"2026-01-13T19:02:59.384449Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2026-01-13 19:02:41.341337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768330961.701942      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768330961.789746      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768330962.638121      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768330962.638161      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768330962.638164      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768330962.638166      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def get_landmarks(DATA_DIR):\n    classes = [\n        d for d in os.listdir(DATA_DIR)\n        if os.path.isdir(os.path.join(DATA_DIR, d))\n    ]\n    num_classes = len(classes)\n\n    for label in classes:\n        label_path = os.path.join(DATA_DIR, label)\n\n        for subject in os.listdir(label_path):\n            subject_path = os.path.join(label_path, subject)\n            if not os.path.isdir(subject_path):\n                continue\n\n            for file in os.listdir(subject_path):\n                if not file.endswith(\".npy\"):\n                    continue\n\n                file_path = os.path.join(subject_path, file)\n\n                try:\n                    data = np.load(file_path)\n                except FileNotFoundError:\n                    print(\"Missing:\", file_path)\n                    continue\n\n                yield data, label, num_classes\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n        super().__init__()\n\n        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.attn = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim // num_heads,  # ğŸ”¥ FIX\n            dropout=dropout\n        )\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n\n        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            tf.keras.layers.Dense(mlp_dim, activation=tf.nn.gelu),\n            tf.keras.layers.Dropout(dropout),\n            tf.keras.layers.Dense(embed_dim),\n            tf.keras.layers.Dropout(dropout),\n        ])\n\n    def call(self, x, training=False):\n        # Pre-Norm Attention\n        attn_out = self.attn(self.norm1(x), self.norm1(x), training=training)\n        x = x + self.drop1(attn_out, training=training)\n\n        # Pre-Norm MLP\n        x = x + self.mlp(self.norm2(x), training=training)\n        return x\n\ndef segregate_dataset(dataset,length_dataset):\n\n    dataset = dataset.shuffle(\n        buffer_size = length_dataset,\n        seed = 42,\n        reshuffle_each_iteration = False\n    )\n    train_size = int(0.7 * length_dataset)\n    val_size = int(0.15 * length_dataset)\n    test_size = length_dataset - train_size - val_size\n\n    train_ds = dataset.take(train_size)\n    remaining = dataset.skip(train_size)\n\n    val_ds = remaining.take(val_size)\n    test_ds = remaining.skip(val_size)\n\n    yield train_ds, val_ds, test_ds\n\ndef get_shape_ds(dataset):\n    for data, label in dataset.take(1):\n        print(f'Data shape: {data.shape}')\n        print(f'Label shape: {label.shape}')\n\ndef build_attention_extractor(vit_model, encoder_layer):\n    x = vit_model.input\n    for layer in vit_model.layers:\n        if layer == encoder_layer:\n            norm_x = layer.norm1(x)\n            _, attn_scores = layer.attn(\n                norm_x, norm_x, return_attention_scores=True\n            )\n            return tf.keras.Model(vit_model.input, attn_scores)\n        x = layer(x)\n\n# Visualize Attention Maps\ndef visualize_attention(model, dataset, sample_id=0):\n    for x, y in dataset.take(1):\n        sample = x[sample_id:sample_id+1]\n\n    attention_model = tf.keras.Model(\n        inputs=model.input,\n        outputs=model.layers[-3].output  # Transformer layer\n    )\n\n    attn_scores = attention_model(sample, training=False)\n    attn_scores = attn_scores.numpy().mean(axis=1)  # avg heads\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(attn_scores[0], cmap=\"viridis\")\n    plt.colorbar()\n    plt.title(\"Transformer Attention Map\")\n    plt.xlabel(\"Landmark Tokens\")\n    plt.ylabel(\"Landmark Tokens\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:04:53.452763Z","iopub.execute_input":"2026-01-13T19:04:53.453078Z","iopub.status.idle":"2026-01-13T19:04:53.467456Z","shell.execute_reply.started":"2026-01-13T19:04:53.453051Z","shell.execute_reply":"2026-01-13T19:04:53.466579Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_landmarks(DATA_DIR):\n    classes = [\n        d for d in os.listdir(DATA_DIR)\n        if os.path.isdir(os.path.join(DATA_DIR, d))\n    ]\n    num_classes = len(classes)\n\n    for label in classes:\n        label_path = os.path.join(DATA_DIR, label)\n\n        for subject in os.listdir(label_path):\n            subject_path = os.path.join(label_path, subject)\n            if not os.path.isdir(subject_path):\n                continue\n\n            for file in os.listdir(subject_path):\n                if not file.endswith(\".npy\"):\n                    continue\n\n                file_path = os.path.join(subject_path, file)\n\n                try:\n                    data = np.load(file_path)\n                except FileNotFoundError:\n                    print(\"Missing:\", file_path)\n                    continue\n\n                yield data, label, num_classes\n\n\ndef segregate_dataset(dataset,length_dataset):\n\n    dataset = dataset.shuffle(\n        buffer_size = length_dataset,\n        seed = 42,\n        reshuffle_each_iteration = False\n    )\n    train_size = int(0.7 * length_dataset)\n    val_size = int(0.15 * length_dataset)\n    test_size = length_dataset - train_size - val_size\n\n    train_ds = dataset.take(train_size)\n    remaining = dataset.skip(train_size)\n\n    val_ds = remaining.take(val_size)\n    test_ds = remaining.skip(val_size)\n\n    yield train_ds, val_ds, test_ds\n\ndef get_shape_ds(dataset):\n    for data, label in dataset.take(1):\n        print(f'Data shape: {data.shape}')\n        print(f'Label shape: {label.shape}')\n# Here I define the transformer Encoder\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n        super().__init__()\n        self.norm1 = tf.keras.layers.LayerNormalization()\n        self.attn = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim,\n            output_shape=embed_dim\n        )\n        self.dropout1 = tf.keras.layers.Dropout(dropout)\n\n        self.norm2 = tf.keras.layers.LayerNormalization()\n        self.mlp = tf.keras.Sequential([\n            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n            tf.keras.layers.Dropout(dropout),\n            tf.keras.layers.Dense(embed_dim),\n            tf.keras.layers.Dropout(dropout),\n        ])\n\n    def call(self, x, training=False, return_attention=False):\n        attn_output, attn_scores = self.attn(\n            self.norm1(x),\n            self.norm1(x),\n            return_attention_scores=True\n        )\n\n        x = x + self.dropout1(attn_output, training=training)\n        x = x + self.mlp(self.norm2(x), training=training)\n\n        if return_attention:\n            return x, attn_scores\n        return x\n# Extract Attention Scores\ndef build_attention_extractor(vit_model, layer_index=0):\n    return tf.keras.Model(\n        inputs=vit_model.input,\n        outputs=vit_model.layers[layer_index].output\n    )\n\n# Visualize Attention Maps\ndef visualize_attention(model, dataset, sample_id=0):\n    for x, y in dataset.take(1):\n        sample = x[sample_id:sample_id+1]\n\n    attention_model = tf.keras.Model(\n        inputs=model.input,\n        outputs=model.layers[-3].output  # Transformer layer\n    )\n\n    attn_scores = attention_model(sample, training=False)\n    attn_scores = attn_scores.numpy().mean(axis=1)  # avg heads\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(attn_scores[0], cmap=\"viridis\")\n    plt.colorbar()\n    plt.title(\"Transformer Attention Map\")\n    plt.xlabel(\"Landmark Tokens\")\n    plt.ylabel(\"Landmark Tokens\")\n    plt.show()\nclass expand_dim(tf.keras.layers.Layer):\n    def __init__(self, axis):\n        super(expand_dim, self).__init__()\n        self.axis = axis\n\n    def call(self, inputs):\n        return tf.expand_dims(inputs, axis=self.axis)\n\n# Here I define the ViT\ndef build_vit(\n    input_dim,\n    num_classes,\n    num_tokens=1662,     # landmark count\n    embed_dim=128,\n    num_heads=4,\n    mlp_dim=64,\n    num_layers=6,\n    dropout=0.1\n):\n    inputs = tf.keras.Input(shape=(input_dim,))\n\n    # Convert landmark vector â†’ tokens\n    x = tf.keras.layers.Reshape((num_tokens, 1))(inputs)\n    x = tf.keras.layers.Dense(embed_dim)(x)\n\n    # Positional Embedding\n    positions = tf.range(start=0, limit=num_tokens, delta=1)\n    pos_embed = tf.keras.layers.Embedding(\n        input_dim=num_tokens,\n        output_dim=embed_dim\n    )(positions)\n\n    x = x + pos_embed\n\n    # Transformer Encoder Stack\n    for _ in range(num_layers):\n        x = TransformerEncoder(\n            embed_dim, num_heads, mlp_dim, dropout\n        )(x)\n\n    # Classification Head\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    outputs = tf.keras.layers.Dense(\n        num_classes,\n        activation=\"softmax\"\n    )(x)\n    exp_dim = expand_dim(axis=1)\n    outputs = exp_dim(outputs)\n\n    model = tf.keras.Model(inputs, outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:05:01.826757Z","iopub.execute_input":"2026-01-13T19:05:01.827058Z","iopub.status.idle":"2026-01-13T19:05:01.846781Z","shell.execute_reply.started":"2026-01-13T19:05:01.827034Z","shell.execute_reply":"2026-01-13T19:05:01.845938Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def build_vit(\n    num_classes,\n    num_landmarks=554,     # 1662 / 3\n    embed_dim=128,\n    num_heads=4,\n    mlp_dim=256,\n    num_layers=4,\n    dropout=0.1\n):\n    inputs = tf.keras.Input(shape=(num_landmarks, 3))\n\n    # Linear projection of landmarks\n    x = tf.keras.layers.Dense(embed_dim)(inputs)\n\n    # CLS token\n    cls_token = tf.Variable(\n        initial_value=tf.zeros((1, 1, embed_dim)),\n        trainable=True,\n        name=\"cls_token\"\n    )\n    batch_size = tf.shape(x)[0]\n    cls_tokens = tf.tile(cls_token, [batch_size, 1, 1])\n    x = tf.concat([cls_tokens, x], axis=1)\n\n    # Positional Embedding\n    pos_embed = tf.keras.layers.Embedding(\n        input_dim=num_landmarks + 1,\n        output_dim=embed_dim\n    )\n    positions = tf.range(start=0, limit=num_landmarks + 1)\n    x = x + pos_embed(positions)\n\n    # Transformer blocks\n    for _ in range(num_layers):\n        x = TransformerEncoder(\n            embed_dim, num_heads, mlp_dim, dropout\n        )(x)\n\n    # CLS token output\n    x = x[:, 0]\n    x = tf.keras.layers.LayerNormalization()(x)\n\n    outputs = tf.keras.layers.Dense(num_classes)(x)  # NO softmax\n\n    model = tf.keras.Model(inputs, outputs)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:05:10.577147Z","iopub.execute_input":"2026-01-13T19:05:10.577758Z","iopub.status.idle":"2026-01-13T19:05:10.584247Z","shell.execute_reply.started":"2026-01-13T19:05:10.577729Z","shell.execute_reply":"2026-01-13T19:05:10.583428Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_landmarks(DATA_DIR):\n    classes = [\n        d for d in os.listdir(DATA_DIR)\n        if os.path.isdir(os.path.join(DATA_DIR, d))\n    ]\n    num_classes = len(classes)\n\n    for label in classes:\n        label_path = os.path.join(DATA_DIR, label)\n\n        for subject in os.listdir(label_path):\n            subject_path = os.path.join(label_path, subject)\n            if not os.path.isdir(subject_path):\n                continue\n\n            for file in os.listdir(subject_path):\n                if not file.endswith(\".npy\"):\n                    continue\n\n                file_path = os.path.join(subject_path, file)\n\n                try:\n                    data = np.load(file_path)\n                except FileNotFoundError:\n                    print(\"Missing:\", file_path)\n                    continue\n\n                yield data, label, num_classes\n\n\ndef segregate_dataset(dataset,length_dataset):\n\n    dataset = dataset.shuffle(\n        buffer_size = length_dataset,\n        seed = 42,\n        reshuffle_each_iteration = False\n    )\n    train_size = int(0.7 * length_dataset)\n    val_size = int(0.15 * length_dataset)\n    test_size = length_dataset - train_size - val_size\n\n    train_ds = dataset.take(train_size)\n    remaining = dataset.skip(train_size)\n\n    val_ds = remaining.take(val_size)\n    test_ds = remaining.skip(val_size)\n\n    yield train_ds, val_ds, test_ds\n\ndef get_shape_ds(dataset):\n    for data, label in dataset.take(1):\n        print(f'Data shape: {data.shape}')\n        print(f'Label shape: {label.shape}')\n# Here I define the transformer Encoder\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n        super().__init__()\n        self.norm1 = tf.keras.layers.LayerNormalization()\n        self.attn = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim,\n            output_shape=embed_dim\n        )\n        self.dropout1 = tf.keras.layers.Dropout(dropout)\n\n        self.norm2 = tf.keras.layers.LayerNormalization()\n        self.mlp = tf.keras.Sequential([\n            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n            tf.keras.layers.Dropout(dropout),\n            tf.keras.layers.Dense(embed_dim),\n            tf.keras.layers.Dropout(dropout),\n        ])\n\n    def call(self, x, training=False, return_attention=False):\n        attn_output, attn_scores = self.attn(\n            self.norm1(x),\n            self.norm1(x),\n            return_attention_scores=True\n        )\n\n        x = x + self.dropout1(attn_output, training=training)\n        x = x + self.mlp(self.norm2(x), training=training)\n\n        if return_attention:\n            return x, attn_scores\n        return x\n# Extract Attention Scores\ndef build_attention_extractor(vit_model, layer_index=0):\n    return tf.keras.Model(\n        inputs=vit_model.input,\n        outputs=vit_model.layers[layer_index].output\n    )\n\n# Visualize Attention Maps\ndef visualize_attention(model, dataset, sample_id=0):\n    for x, y in dataset.take(1):\n        sample = x[sample_id:sample_id+1]\n\n    attention_model = tf.keras.Model(\n        inputs=model.input,\n        outputs=model.layers[-3].output  # Transformer layer\n    )\n\n    attn_scores = attention_model(sample, training=False)\n    attn_scores = attn_scores.numpy().mean(axis=1)  # avg heads\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(attn_scores[0], cmap=\"viridis\")\n    plt.colorbar()\n    plt.title(\"Transformer Attention Map\")\n    plt.xlabel(\"Landmark Tokens\")\n    plt.ylabel(\"Landmark Tokens\")\n    plt.show()\nclass expand_dim(tf.keras.layers.Layer):\n    def __init__(self, axis):\n        super(expand_dim, self).__init__()\n        self.axis = axis\n\n    def call(self, inputs):\n        return tf.expand_dims(inputs, axis=self.axis)\n\n# Here I define the ViT\ndef build_vit(\n    input_dim,\n    num_classes,\n    num_tokens=1662,     # landmark count\n    embed_dim=128,\n    num_heads=4,\n    mlp_dim=64,\n    num_layers=6,\n    dropout=0.1\n):\n    inputs = tf.keras.Input(shape=(input_dim,))\n\n    # Convert landmark vector â†’ tokens\n    x = tf.keras.layers.Reshape((num_tokens, 1))(inputs)\n    x = tf.keras.layers.Dense(embed_dim)(x)\n\n    # Positional Embedding\n    positions = tf.range(start=0, limit=num_tokens, delta=1)\n    pos_embed = tf.keras.layers.Embedding(\n        input_dim=num_tokens,\n        output_dim=embed_dim\n    )(positions)\n\n    x = x + pos_embed\n\n    # Transformer Encoder Stack\n    for _ in range(num_layers):\n        x = TransformerEncoder(\n            embed_dim, num_heads, mlp_dim, dropout\n        )(x)\n\n    # Classification Head\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    outputs = tf.keras.layers.Dense(\n        num_classes,\n        activation=\"softmax\"\n    )(x)\n    exp_dim = expand_dim(axis=1)\n    outputs = exp_dim(outputs)\n\n    model = tf.keras.Model(inputs, outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:05:15.847826Z","iopub.execute_input":"2026-01-13T19:05:15.848118Z","iopub.status.idle":"2026-01-13T19:05:15.864500Z","shell.execute_reply.started":"2026-01-13T19:05:15.848093Z","shell.execute_reply":"2026-01-13T19:05:15.863622Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if __name__ == '__main__':\n    DATA_DIR = '/kaggle/input/auslan/AUSLAN_Data'\n    results = get_landmarks(DATA_DIR)\n    print(results)\n    composite_features, alphabet, num_classes = zip(*results)\n    composite_features = np.array(composite_features)\n    alphabet = np.array(alphabet)\n    num_classes = np.max(num_classes)\n    # lets generate a tensorflow dataset\n    dataset = tf.data.Dataset.from_tensor_slices((composite_features, alphabet\n                                                  ))\n    # Ok now we have the dataset so now we can segregate it into\n    # Training, validation and test datasets\n    # Lets do that\n    # But we do so by declaring a function\n    # But First we need to convert the label i-e alphabet into one hot encoding\n    label_lookup = tf.keras.layers.StringLookup(output_mode = \"int\")\n    label_lookup.adapt(alphabet)\n    #num_classes = label_lookup.vocabulary_size()\n    one_hot_encoder = tf.keras.layers.CategoryEncoding(\n        num_tokens = num_classes,\n        output_mode = \"one_hot\"\n    )\n    dataset = dataset.map(\n        lambda x,y: (x, one_hot_encoder(label_lookup(y)-1)),\n        num_parallel_calls = tf.data.AUTOTUNE\n    )\n    segregated_dataset = segregate_dataset(dataset, len(composite_features))\n    train_ds, val_ds, test_ds = next(segregated_dataset)\n\n    # Now we convert the datasets into batches\n    BATCH_SIZE = 32\n    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    # Lets check the shape of the datasets\n    # get_shape_ds(train_ds)\n    # get_shape_ds(val_ds)\n    # get_shape_ds(test_ds)\n    INPUT_DIM = composite_features.shape[1]\n    NUM_CLASSES = num_classes\n    # Lets Build the ViT Model\n    vit_model = build_vit(\n        input_dim=INPUT_DIM,\n        num_classes=NUM_CLASSES\n    )\n    # Here we compile our ViT Model\n    vit_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    vit_model.summary()\n    # Now we define the Model Checkpoint Callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"vit_best_model.keras\",\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1\n    )\n    # Defining our CSV logger\n    csv_logger_cb = tf.keras.callbacks.CSVLogger(\n    filename=\"/kaggle/working/vit_training_log.csv\",\n    append=True\n    )\n    # Defining Early Stopping Callback\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=10,\n    restore_best_weights=True\n    )\n    callbacks = [\n    checkpoint_cb,\n    csv_logger_cb,\n    early_stopping_cb\n    ]\n    # Now training the Model\n    print(\"Training of the model starts here...\")\n    history = vit_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    callbacks=callbacks\n    )\n    test_loss, test_acc = vit_model.evaluate(test_ds)\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n\n    y_true = []\n    y_pred = []\n\n    for x, y in test_ds:\n        preds = vit_model.predict(x)\n        y_true.extend(np.argmax(y.numpy(), axis=1))\n        y_pred.extend(np.argmax(preds, axis=1))\n\n\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\"\n    )\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix for AUSLAN Classification\")\n    plt.savefig(\"/kaggle/working//Confusion.png\")\n    plt.show()\n\n    print(classification_report(\n    y_true,\n    y_pred,\n    digits=4))\n\n    precision, recall, f1, support = precision_recall_fscore_support(\n    y_true,\n    y_pred,\n    average=\"macro\"\n    )\n\n    print(f\"Macro Precision: {precision:.4f}\")\n    print(f\"Macro Recall: {recall:.4f}\")\n    print(f\"Macro F1-score: {f1:.4f}\")\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n    plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n    plt.legend()\n    plt.title(\"Accuracy Curve\")\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n    plt.legend()\n    plt.title(\"Loss Curve\")\n    plt.savefig(\"/kaggle/working//LossCurve.png\")\n    plt.show()\n\n\n    ## ROC AUC Curve\n    y_true_bin = label_binarize(y_true, classes=range(NUM_CLASSES))\n    y_pred_prob = np.array([\n    vit_model.predict(x) for x, _ in test_ds\n    ]).reshape(len(y_true), NUM_CLASSES)\n\n    for i in range(NUM_CLASSES):\n        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f\"Class {i} (AUC={roc_auc:.2f})\")\n\n    plt.legend()\n    plt.title(\"ROC Curves\")\n    plt.savefig(\"/kaggle/working//RoC.png\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:05:23.785002Z","iopub.execute_input":"2026-01-13T19:05:23.785791Z"}},"outputs":[{"name":"stdout","text":"<generator object get_landmarks at 0x781f6ea67d80>\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1768331244.106896      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ reshape (\u001b[38;5;33mReshape\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m1\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)                       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_4           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_5           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m280,896\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_12          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             â”‚         \u001b[38;5;34m3,354\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ expand_dim (\u001b[38;5;33mexpand_dim\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m26\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_4           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_encoder_5           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,896</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_12          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ expand_dim (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">expand_dim</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,689,242\u001b[0m (6.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,689,242</span> (6.44 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,689,242\u001b[0m (6.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,689,242</span> (6.44 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Training of the model starts here...\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1768331266.926153     122 service.cc:152] XLA service 0x781e840021c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1768331266.926198     122 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1768331269.988271     122 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1768331290.055322     122 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m611/854\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”\u001b[0m \u001b[1m2:55\u001b[0m 721ms/step - accuracy: 0.0398 - loss: 3.3134","output_type":"stream"}],"execution_count":null}]}